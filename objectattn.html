<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link rel="shortcut icon" href="./Images/favicon.png">

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}

.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Sean Noah</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="about.html">About Me</a>
</li>
<li>
  <a href="science.html">Science</a>
</li>
<li>
  <a href="writing.html">Writing</a>
</li>
<li>
  <a href="cv.html">CV</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">Contact</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<h2>
<br> The neural mechanisms of object-based attention
</h2>
<br> <br> In the Mangun lab, we’re studying the neural mechanisms of attention, which means in part that we’re trying to answer the question: <b>How does the brain <i>select</i> sensory information for privileged access to cognition and perception?</b> To address that question, we’re working on a model of attention called the specificity of control – or SpOC – model. <br> <br> <img src="Images/ObjectAttn/spocdiagram.png" /> <br>
<center>
<i>The SpOC (pronounced “spoke”) model of attention compared with alterantives. Mangun, unpublished</i>
</center>
<br> <br> According to this model, the dorsal attention network (DAN), which is a group of areas in frontal and parietal cortices that have previously been associated with voluntary selective attention, is made up of <b>many smaller-scale networks that each are functionally connected to a specific sensory area</b>. These specific connections could serve as channels through which executive control areas exert their influence, and <b>selectively enhance or suppress sensory neural activity</b>. In this way, the DAN would act as an interface between sensory areas and upstream executive signals. <br> <br> Recently I’ve been working on a possible extension of the SpOC model: Not only do the assembly-level networks in the DAN each connect to specific sensory areas, but they also <b>operate on those sensory areas by the same mechanism</b>. We think that mechanism involves modulation of alpha band, or 8-12 Hz, oscillatory neural activity. Much previous work on spatial attention and some work on feature-based attention has shown that the power of alpha band oscillations decreases in brain areas responsive to the attended region of space, or low-level visual feature. This body of findings, together with the SpOC model, leads us to the hypothesis that <b>if DAN subnetworks operate on sensory areas by the same mechanism, then we should see alpha modulation during attention to other classes of visual targets that have their own dedicated processing areas.</b> <br> <br> Two such dedicated processing areas that have not previously been studied in the context of functional inhibition by alpha are the fusiform face area (FFA) and the parahippocampal place area (PPA). Both areas have been extensively characterized by Nancy Kanwisher and her colleagues. The FFA is a bilateral, but right-emphasized area of the inferior temporal lobe that is highly responsive to face images. The PPA is a bilateral, medial temporal area that is highly responsive to images of places or scenes, although there is still much to figure out about exactly what kind of place information the PPA is responsive to. Anyway, the point is that these are notable examples of high level visual areas that, under the SpOC model, would be targets of influence from specific assembly-level networks within the DAN, subserving <b>object-based attention</b>. I’m going to classify these areas as object representation areas, because they respond to more complex and more holisitic visual information than what we typically think of as visual features such as motion, color, and orientation. <br> <br> A quick aside on terminology: what I refer to as “object-based attention” is quite distinct from the term as it appears in work derived from Egly’s studies of same-object advantage. In the Egly terminology, visual continuity properties seem to facilitate the automatic spreading of attention from cued to same-object locations, but not to different-object locations just as far from the cue. In my use of the term “object-based attention,” I’m referring to a voluntary, selective enhancement of a cued object category, such as cars, cats, faces, or houses. <b>This form of selection can also be thought of as attention to high-level visual features.</b> <br> <br> This is a subtle form of attention, less immediately relatable than something like covert spatial attention. So I’ll provide an ecological example from my own life. <br> <br> Over the last few weeks of heavy rains, some little critter has taken up residence in my attic. Every night I’ve been hearing it rustling around above my head, although thankfully separated from me by a thick layer of asbestos, until now, when I hear it crash its way into my furnace closet. <br> <br> I know it’s some kind of small animal, but I want to know exactly what kind. Once I fling open the closet door, I need to be able to identify it as quickly and accurately as possible, so that I can execute the appropriate response. <br> <br>
<center>
<img src="Images/ObjectAttn/sq.gif" />
</center>
<br> <br> It turns out it’s a squirrel, so I slam the door closed before it leaps out into my house, and not with a fraction of second to spare. <br> <br> This squirrel-based example illustrates the form of attention that we’re interested in capturing with an experimental paradigm. So we designed this simple task for participants to do while we record their EEG: <br> <br>
<center>
<img src="Images/ObjectAttn/paradigm.png" />
</center>
<center>
<i>Diagram of object-based attention paradigm</i>
</center>
<br> <br> We gave all participants a controller with two buttons, and told them: Press button 1 if target image is male face, natural place, or powered tool; press button 2 if target image is female face, urban place, or hand tool; do it as fast and accurately as you can. Crucially, participants were trained on the relationship between <b>cue</b> shape and <b>target</b> object category. A triangle indicated the upcoming appearance of a face. A square indicated the upcoming appearance of a place. A circle indicated the upcoming appearance of a tool. The cue validity was 80%, meaning that 20% of the time, the cue shape would mislead particiapnts and a target image from an uncued object category would appear. This cue validity-based design allowed us to contrast performance from trials where participants were anticipating the target object category against trials where participants were anticipating a <i>different</i> object category than the one presented. All stimuli were presented at center fixation, so eye movements and spatial attention would be kept minimal and consistent across all participants and conditions. We recorded EEG data from 20 undergraduate participants performing this experiment, with about 400 trials for each participant. <br> <br> We hoped that on any given validly-cued trial, after seeing the cue, participants would engage object-based attention for the cued category of object, which would enable them to perform the required identification (male vs. female, natural vs. urban, powered vs. non-powered) faster and more accurately than on invalidly-cued trials. It’s typical in attention research to check for an attention effect by looking at behavioral data such as reaction time (RT) or accuracy. Trials in which participants are actively attending to an upcoming target exhibit better performance: faster reaction time and higher accuracy. When we looked at RT across all our oject categories, we saw statistically significant differences between valid trials and invalid trials, confirming our expectation that participants were utilizing object-based attention. <br> <br>
<center>
<img src="Images/ObjectAttn/full_behav_boxplot.png" />
</center>
<center>
<i>Reaction times were faster for validly-cued trials than invalidly-cued trials.</i>
</center>
<br> <br> With behavioral data supporting the assertion that our experimental paradigm was engaging object-based attention, we proceeded to our main question: Does EEG recorded during episodes of object-based attention show alpha band modulation, analogously with spatial and feature-based attention? <br> <br> As a first pass, we plotted differences in alpha power, averaged across all 20 participants, between conditions on topographic scalp maps. These topomaps are snapshots of instantaneous alpha power distributions across the scalp at various time points in the experimental epoch (500 msec before the cue onset to 1000 msec after the cue onset). With these figures, we can see that there are differences in alpha topography that depend on whether one is attending to faces, places, or tools. Moreover, there are some alpha power lateralizations that we thought would occur, given our hypothesis that alpha is down-modulated within visual areas that respond to an attended object category. For instance, the FFA is right-emphasized compared to the PPA, so for the attend face minus attend place contrast, we expected to see greater alpha power over left hemisphere than right hemisphere. This is indeed what we see, especially toward the end of the epoch. <br> <br>
<center>
<img src="Images/headplot_alpha_face_minus_scene.jpg" />
</center>
<center>
<i>Alpha topography over the epoch: attend face minus attend scene</i>
</center>
<br> <br>
<center>
<img src="Images/headplot_alpha_face_minus_tool.jpg" />
</center>
<center>
<i>Alpha topography over the epoch: attend face minus attend tool</i>
</center>
<br> <br>
<center>
<img src="Images/headplot_alpha_tool_minus_scene.jpg" />
</center>
<center>
<i>Alpha topography over the epoch: attend tool minus attend scene</i>
</center>
<br> <br> However, seeing differences in alpha power between conditions is not enough to know what differences are meaningful (e.g., statistically significant, consistent with prior knowledge, etc.). For a more robust approach to our alpha power analysis, we adopted a classifier, a.k.a decoding, strategy. Essentially, we set out to train a classifier model on a subset of alpha power data, labeled as to what kind of trial it came from (attend face, attend, scene, or attend tool). Then, we tested the model on unlabeled data, asking it to provide its best guess as to what kind of trial each datapoint of unlabeled data belonged. The logic behind this type of analysis is that if there is information in the alpha power topography that links a given time point to a specific attended object category, then the classifer would be able to perform above chance. Because we perform classification at each time point in the epoch, we are able to infer not only how much the alpha topography is modulated by attention to different object categories, but when, with high temporal precision, these modulations diverge from one another. <br> <br> For our analysis, we modified the procedure described in Bae and Luck, 2018. For each participant and time point in the epoch, we performed 10 iterations of 6-fold cross validation, training a support vector machine (SVM) learner using a one vs. one error correcting output codes (ECOC) coding scheme. To determine statistical significance of the classifier’s accuracy at each time point, we ran a one-tailed, one-sample t test of decoding accuracy for all subjects against chance (1/3). Then, to circumvent multiple comparisons issues, we used a non-parametric method to determine statistically significant size of a <i>cluster</i> of significant t test results. The logic in this approach is that real brain data linked to forms of cognition like we are studying here shouldn’t change dramatically from millisecond to millisecond, so any real differences between conditions should persist as clusters over time. We simulated a decoding accuracy dataset using random sampling with replacement of our real decoding accuracies, and then over 1000 iterations, constructed a null distribution of t masses. Essentially, this involved taking a fake dataset, assigning “decoding” values to this dataset at chance level, performing t tests at ever time point in this fake decoding set, and collecting sizes of clusters of significant results. Performing this routine 1000 times allowed us to generate a distriution of cluster t masses that could be expected entirely by chance. Then we were able to determine the critical t mass value, the traditional 95% mark for a one-tailed t test, above which we could safely say that a cluster of t values from our real decoding data is statistically significant. <br> <br>
<center>
<img src="Images/decoding_avg.png" />
</center>
<center>
<i>Classifier accuracy over the epoch, averaged over all participants. Blue dots denote data points belonging to statistically significant clusters.</i>
</center>
<br> <br> Our classifier results showed us that there statistically significant clusters of decoding in the 500-800 msec range of the epoch. This means that in that time range, <b>alpha power across our object category conditions was different enough that the classifer could be trained to identify the object that was being attended, significantly above chance</b>. <br> <br>
<center>
<img src="Images/correlation_avg.png" />
</center>
<center>
<i>Correlation between magnitude of behavioral attention effect and decoding accuracy</i>
</center>
<br> <br> Moreover, we found that <b>the greater the behavioral attention effect per participant, the greater the decoding accuracy</b>. In other words, the more effectively a participant was able to focus their attention to a specific object category before the target image actually appeared, the faster they were able to respond to that target image, and the more distinct their alpha topography was across the object conditions. This positive correlation nicely corroborated our basic hypothesis that attending to different object categories modulates alpha power in different brain areas in ways that will be discernible at the level of the scalp. <br> <br>
<center>
<img src="Images/decode_acc.gif" />
</center>
<center>
<i>Decoding accuracy with increasing RT effect threshold for inclusion</i>
</center>
<p><br> <br></p>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
